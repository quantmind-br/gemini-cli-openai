```diff
diff --git a/.dev.vars.example b/.dev.vars.example
deleted file mode 100644
index 455f89e..0000000
--- a/.dev.vars.example
+++ /dev/null
@@ -1,29 +0,0 @@
-# Gemini CLI OpenAI Worker Environment Variables
-
-# Required: OAuth2 credentials JSON from Gemini CLI authentication
-# Get this by running `gemini auth` and copying the contents of ~/.gemini/oauth_creds.json
-GCP_SERVICE_ACCOUNT={"access_token":"ya29.a0AS3H6Nx...","refresh_token":"1//09FtpJYpxOd...","scope":"https://www.googleapis.com/auth/cloud-platform ...","token_type":"Bearer","id_token":"eyJhbGciOiJSUzI1NiIs...","expiry_date":1750927763467}
-
-# Optional: Google Cloud Project ID (auto-discovered if not set)
-# GEMINI_PROJECT_ID=your-project-id
-
-# Optional: API key for authentication (if not set, API is public)
-# When set, clients must include "Authorization: Bearer <your-api-key>" header
-# Example: sk-1234567890abcdef1234567890abcdef
-OPENAI_API_KEY=sk-your-secret-api-key-here
-
-# Optional: Enable fake thinking output for thinking models (set to "true" to enable)
-# When enabled, models marked with thinking: true will generate synthetic reasoning text
-# before providing their actual response, similar to OpenAI's o3 model behavior
-ENABLE_FAKE_THINKING=true
-
-# Optional: Enable real Gemini thinking output (set to "true" to enable)
-# When enabled, requests with include_reasoning=true will use Gemini's native thinking
-# This requires thinking-capable models and provides genuine reasoning from Gemini
-ENABLE_REAL_THINKING=true
-
-# Optional: Stream thinking as content with <thinking> tags (DeepSeek R1 style)
-# When enabled along with either thinking mode, reasoning will be streamed as regular content
-# wrapped in <thinking></thinking> tags instead of using the reasoning field
-STREAM_THINKING_AS_CONTENT=true
diff --git a/.env.example b/.env.example
new file mode 100644
index 0000000..892410a
--- /dev/null
+++ b/.env.example
@@ -0,0 +1,32 @@
+# Gemini CLI OpenAI Docker Environment Variables
+
+# Port for the application to listen on
+PORT=3000
+
+# Redis connection URL for caching
+REDIS_URL=redis://redis:6379
+
+# Required: OAuth2 credentials JSON from Gemini CLI authentication
+# Get this by running `gemini auth` and copying the contents of ~/.gemini/oauth_creds.json
+# IMPORTANT: This should be a single line with no line breaks.
+GCP_SERVICE_ACCOUNT='{"access_token":"ya29.a0AS3H6Nx...","refresh_token":"1//09FtpJYpxOd...","scope":"https://www.googleapis.com/auth/cloud-platform ...","token_type":"Bearer","id_token":"eyJhbGciOiJSUzI1NiIs...","expiry_date":1750927763467}'
+
+# Optional: Google Cloud Project ID (auto-discovered if not set)
+# GEMINI_PROJECT_ID=your-project-id
+
+# Optional: API key for authentication (if not set, API is public)
+# When set, clients must include "Authorization: Bearer <your-api-key>" header
+# Example: sk-1234567890abcdef1234567890abcdef
+OPENAI_API_KEY=sk-your-secret-api-key-here
+
+# Optional: Enable fake thinking output for thinking models (set to "true" to enable)
+# When enabled, models marked with thinking: true will generate synthetic reasoning text
+# before providing their actual response, similar to OpenAI's o3 model behavior
+ENABLE_FAKE_THINKING=true
+
+# Optional: Enable real Gemini thinking output (set to "true" to enable)
+# When enabled, requests with include_reasoning=true will use Gemini's native thinking
+# This requires thinking-capable models and provides genuine reasoning from Gemini
+ENABLE_REAL_THINKING=true
+
+# Optional: Stream thinking as content with <thinking> tags (DeepSeek R1 style)
+# When enabled along with either thinking mode, reasoning will be streamed as regular content
+# wrapped in <thinking></thinking>tags instead of using the reasoning field
+STREAM_THINKING_AS_CONTENT=true
diff --git a/.github/workflows/deploy.yml b/.github/workflows/deploy.yml
deleted file mode 100644
index 4557209..0000000
--- a/.github/workflows/deploy.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-name: Deploy Cloudflare Worker
-
-on:
-  push:
-    branches:
-      - main
-
-jobs:
-  deploy:
-    runs-on: ubuntu-latest
-    name: Deploy
-    steps:
-      - uses: actions/checkout@v4
-      - name: Deploy
-        uses: cloudflare/wrangler-action@v3
-        with:
-          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
diff --git a/Dockerfile b/Dockerfile
new file mode 100644
index 0000000..f265842
--- /dev/null
+++ b/Dockerfile
@@ -0,0 +1,17 @@
+# Stage 1: Build the application
+FROM node:22-alpine AS builder
+WORKDIR /app
+COPY package.json yarn.lock ./
+RUN yarn install --frozen-lockfile
+COPY . .
+RUN yarn build
+
+# Stage 2: Create the production image
+FROM node:22-alpine
+WORKDIR /app
+COPY --from=builder /app/package.json /app/yarn.lock ./
+RUN yarn install --production --frozen-lockfile
+COPY --from=builder /app/dist ./dist
+
+EXPOSE 3000
+CMD ["node", "dist/index.js"]
diff --git a/README.md b/README.md
index 10a8276..0041d8b 100644
--- a/README.md
+++ b/README.md
@@ -1,19 +1,19 @@
-# üöÄ Gemini CLI OpenAI Worker
+# üöÄ Gemini CLI OpenAI
 
 [!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://www.buymeacoffee.com/mrproper)
 
-Transform Google's Gemini models into OpenAI-compatible endpoints using Cloudflare Workers. Access Google's most advanced AI models through familiar OpenAI API patterns, powered by OAuth2 authentication and the same infrastructure that drives the official Gemini CLI.
+Transform Google's Gemini models into OpenAI-compatible endpoints. Access Google's most advanced AI models through familiar OpenAI API patterns, powered by OAuth2 authentication and the same infrastructure that drives the official Gemini CLI.
 
 ## ‚ú® Features
 
 - üîê **OAuth2 Authentication** - No API keys required, uses your Google account
 - üéØ **OpenAI-Compatible API** - Drop-in replacement for OpenAI endpoints
 - üìö **OpenAI SDK Support** - Works with official OpenAI SDKs and libraries
+- üê≥ **Docker Support** - Easy deployment with Docker and Docker Compose
 - üñºÔ∏è **Vision Support** - Multi-modal conversations with images (base64 & URLs)
 - üåê **Third-party Integration** - Compatible with Open WebUI, ChatGPT clients, and more
-- ‚ö° **Cloudflare Workers** - Global edge deployment with low latency
-- üîÑ **Smart Token Caching** - Intelligent token management with KV storage
+- ‚ö° **High Performance** - Built with Hono and Node.js for speed
+- üîÑ **Smart Token Caching** - Intelligent token management with Redis
 - üÜì **Free Tier Access** - Leverage Google's free tier through Code Assist API
 - üì° **Real-time Streaming** - Server-sent events for live responses
 - üé≠ **Multiple Models** - Access to latest Gemini models including experimental ones
@@ -38,19 +38,19 @@
 > 
 > Set `STREAM_THINKING_AS_CONTENT=true` to stream reasoning as content with `<thinking>` tags (DeepSeek R1 style) instead of using the reasoning field.
 
-## üõ†Ô∏è Setup
+## üê≥ Docker Setup
 
 ### Prerequisites
 
 1. **Google Account** with access to Gemini
-2. **Cloudflare Account** with Workers enabled
-3. **Wrangler CLI** installed (`npm install -g wrangler`)
+2. **Docker** and **Docker Compose** installed
 
 ### Step 1: Get OAuth2 Credentials
 
 You need OAuth2 credentials from a Google account that has accessed Gemini. The easiest way to get these is through the official Gemini CLI.
 
 #### Using Gemini CLI
+
 1. **Install Gemini CLI**:
    ```bash
    npm install -g @google/gemini-cli
@@ -87,60 +87,42 @@
    }
    ```
 
-### Step 2: Create KV Namespace
-
-```bash
-# Create a KV namespace for token caching
-wrangler kv namespace create "GEMINI_CLI_KV"
-```
-
-Note the namespace ID returned.
-Update `wrangler.toml` with your KV namespace ID:
-```toml
-kv_namespaces = [
-  { binding = "GEMINI_CLI_KV", id = "your-kv-namespace-id" }
-]
-```
-
-### Step 3: Environment Setup
-
-Create a `.dev.vars` file:
-```bash
-# Required: OAuth2 credentials JSON from Gemini CLI authentication
-GCP_SERVICE_ACCOUNT={"access_token":"ya29...","refresh_token":"1//...","scope":"...","token_type":"Bearer","id_token":"eyJ...","expiry_date":1750927763467}
-
-# Optional: Google Cloud Project ID (auto-discovered if not set)
-# GEMINI_PROJECT_ID=your-project-id
-
-# Optional: API key for authentication (if not set, API is public)
-# When set, clients must include "Authorization: Bearer <your-api-key>" header
-# Example: sk-1234567890abcdef1234567890abcdef
-OPENAI_API_KEY=sk-your-secret-api-key-here
-```
-
-For production, set the secrets:
-```bash
-wrangler secret put GCP_SERVICE_ACCOUNT
-wrangler secret put OPENAI_API_KEY  # Optional, only if you want authentication
-```
-
-### Step 4: Deploy
+### Step 2: Environment Setup
+
+1. Clone this repository.
+2. Create a `.env` file by copying the example:
+   ```bash
+   cp .env.example .env
+   ```
+3. Open the `.env` file and paste your single-line OAuth2 credentials JSON into the `GCP_SERVICE_ACCOUNT` variable.
+4. Configure other variables like `OPENAI_API_KEY` if needed.
+
+### Step 3: Run with Docker Compose
 
 ```bash
-# Install dependencies
-npm install
-
-# Deploy to Cloudflare Workers
-npm run deploy
-
-# Or run locally for development
-npm run dev
+# Start the application and Redis
+docker-compose up -d
 ```
 
+Your Gemini OpenAI compatible API is now running at `http://localhost:3000`.
+
+### Building and Pushing to Docker Hub
+
+Scripts are provided to build and push the Docker image to Docker Hub.
+
+**For Linux/macOS:**
+```bash
+./build-and-push.sh
+```
+
+**For Windows:**
+```batch
+.\build-and-push.bat
+```
+
 ## üîß Configuration
 
 ### Environment Variables
 
 | Variable | Required | Description |
 |----------|----------|-------------|
-| `GCP_SERVICE_ACCOUNT` | ‚úÖ | OAuth2 credentials JSON string |
+| `GCP_SERVICE_ACCOUNT` | ‚úÖ | OAuth2 credentials JSON string (single line) |
 | `GEMINI_PROJECT_ID` | ‚ùå | Google Cloud Project ID (auto-discovered if not set) |
 | `OPENAI_API_KEY` | ‚ùå | API key for authentication (if not set, API is public) |
 | `ENABLE_FAKE_THINKING` | ‚ùå | Enable synthetic thinking output for thinking models (set to "true" to enable) |
@@ -148,6 +130,8 @@
 | `STREAM_THINKING_AS_CONTENT` | ‚ùå | Stream thinking as content with `<thinking>` tags (DeepSeek R1 style) |
+| `PORT` | ‚ùå | Port for the server to listen on (default: `3000`) |
+| `REDIS_URL` | ‚ùå | Connection URL for Redis cache (default: `redis://redis:6379`) |
 
 **Authentication Security:**
 - When `OPENAI_API_KEY` is set, all `/v1/*` endpoints require authentication
@@ -167,12 +151,6 @@
 - **Optimized UX**: The `</thinking>` tag is only sent when the actual LLM response begins, eliminating awkward pauses between thinking and response
 - If neither thinking mode is enabled, thinking models will behave like regular models
 
-### KV Namespaces
-
-| Binding | Purpose |
-|---------|---------|
-| `GEMINI_CLI_KV` | Token caching and session management |
-
 ## üö® Troubleshooting
 
 ### Common Issues
@@ -197,7 +175,7 @@
 2. **Configure OpenAI API settings**:
    - Open Cline settings
    - Set **API Provider** to "OpenAI"
-   - Set **Base URL** to: `https://your-worker.workers.dev/v1`
+   - Set **Base URL** to: `http://localhost:3000/v1`
    - Set **API Key** to: `sk-your-secret-api-key-here` (use your OPENAI_API_KEY if authentication is enabled)
 
 3. **Select a model**:
@@ -208,7 +186,7 @@
 
 1. **Add as OpenAI-compatible endpoint**:
    - Base URL: `https://your-worker.workers.dev/v1`
-   - API Key: `sk-your-secret-api-key-here` (use your OPENAI_API_KEY if authentication is enabled)
+   - API Key: `your-api-key` (use your OPENAI_API_KEY if authentication is enabled)
 
 2. **Configure models**:
    Open WebUI will automatically discover available Gemini models through the `/v1/models` endpoint.
@@ -222,7 +200,7 @@
 ```python
 import litellm
 
-# Configure LiteLLM to use your worker
+# Configure LiteLLM to use your local server
 litellm.api_base = "https://your-worker.workers.dev/v1"
 litellm.api_key = "sk-your-secret-api-key-here"
 
@@ -245,8 +223,8 @@
 from openai import OpenAI
 
 # Initialize with your worker endpoint
-client = OpenAI(
-    base_url="https://your-worker.workers.dev/v1",
+client = OpenAI( 
+    base_url="http://localhost:3000/v1",
     api_key="sk-your-secret-api-key-here"  # Use your OPENAI_API_KEY if authentication is enabled
 )
 
@@ -288,7 +266,7 @@
 import OpenAI from 'openai';
 
 const openai = new OpenAI({
-  baseURL: 'https://your-worker.workers.dev/v1',
+  baseURL: 'http://localhost:3000/v1',
   apiKey: 'sk-your-secret-api-key-here', // Use your OPENAI_API_KEY if authentication is enabled
 });
 
@@ -306,7 +284,7 @@
 
 ### cURL
 ```bash
-curl -X POST https://your-worker.workers.dev/v1/chat/completions \
+curl -X POST http://localhost:3000/v1/chat/completions \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer sk-your-secret-api-key-here" \
   -d '{
@@ -319,7 +297,7 @@
 
 ### Raw JavaScript/TypeScript
 ```javascript
-const response = await fetch('https://your-worker.workers.dev/v1/chat/completions', {
+const response = await fetch('http://localhost:3000/v1/chat/completions', {
   method: 'POST',
   headers: {
     'Content-Type': 'application/json',
@@ -351,7 +329,7 @@
 import requests
 import json
 
-url = "https://your-worker.workers.dev/v1/chat/completions"
+url = "http://localhost:3000/v1/chat/completions"
 data = {
     "model": "gemini-2.5-flash",
     "messages": [
@@ -374,7 +352,7 @@
 
 ### Base URL
 ```
-https://your-worker.your-subdomain.workers.dev
+http://localhost:3000
 ```
 
 ### List Models
@@ -467,7 +445,7 @@
 
 client = OpenAI(
-    base_url="https://your-worker.workers.dev/v1",
+    base_url="http://localhost:3000/v1",
     api_key="sk-your-secret-api-key-here"
 )
 
@@ -498,7 +476,7 @@
 
 #### Example with Image URL
 ```bash
-curl -X POST https://your-worker.workers.dev/v1/chat/completions \
+curl -X POST http://localhost:3000/v1/chat/completions \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer sk-your-secret-api-key-here" \
   -d '{
@@ -551,28 +529,28 @@
 
 ```bash
 # Check KV cache status
-curl https://your-worker.workers.dev/v1/debug/cache
+curl http://localhost:3000/v1/debug/cache
 
 # Test authentication only
-curl -X POST https://your-worker.workers.dev/v1/token-test
+curl -X POST http://localhost:3000/v1/token-test
 
 # Test full flow
-curl -X POST https://your-worker.workers.dev/v1/test
+curl -X POST http://localhost:3000/v1/test
 ```
 
 
 ## üèóÔ∏è How It Works
 
 ```mermaid
-graph TD
-    A[Client Request] --> B[Cloudflare Worker]
-    B --> C{Token in KV Cache?}
+graph TD;
+    A[Client Request] --> B[Node.js Server (Hono)];
+    B --> C{Token in Redis Cache?};
     C -->|Yes| D[Use Cached Token]
-    C -->|No| E[Check Environment Token]
-    E --> F{Token Valid?}
-    F -->|Yes| G[Cache & Use Token]
-    F -->|No| H[Refresh Token]
-    H --> I[Cache New Token]
-    D --> J[Call Gemini API]
-    G --> J
-    I --> J
-    J --> K[Stream Response]
-    K --> L[OpenAI Format]
-    L --> M[Client Response]
+    C -->|No| E[Check Environment Token];
+    E --> F{Token Valid?};
+    F -->|Yes| G[Cache & Use Token];
+    F -->|No| H[Refresh Token];
+    H --> I[Cache New Token];
+    D --> J[Call Gemini API];
+    G --> J;
+    I --> J;
+    J --> K[Stream Response];
+    K --> L[OpenAI Format];
+    L --> M[Client Response];
 ```
 
 The worker acts as a translation layer, converting OpenAI API calls to Google's Code Assist API format while managing OAuth2 authentication automatically.
@@ -587,11 +565,11 @@
 ## üìÑ License
 
 This project is licensed under the MIT License - see the LICENSE file for details.
 
 ## üôè Acknowledgments
 
 - Inspired by the official [Google Gemini CLI](https://github.com/google-gemini/gemini-cli)
-- Built on [Cloudflare Workers](https://workers.cloudflare.com/)
+- Built with [Node.js](https://nodejs.org/)
 - Uses [Hono](https://hono.dev/) web framework
 
 ---
diff --git a/api-test.http b/api-test.http
index 3277341..b2e557b 100644
--- a/api-test.http
+++ b/api-test.http
@@ -1,23 +1,23 @@
 ### Test the /v1/models endpoint
-GET https://gemini-cli-worker.gewoonjaap.workers.dev/v1/models
+GET http://localhost:3000/v1/models
 Content-Type: application/json
 
 ###
 
 ### Check KV cache status
-GET https://gemini-cli-worker.gewoonjaap.workers.dev/v1/debug/cache
+GET http://localhost:3000/v1/debug/cache
 Content-Type: application/json
 
 ###
 
 ### Test token authentication only
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/token-test
+POST http://localhost:3000/v1/token-test
 Content-Type: application/json
 
 ###
 
 ### Test authentication and basic functionality
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/test
+POST http://localhost:3000/v1/test
 Content-Type: application/json
 
 ###
@@ -25,7 +25,7 @@
 ### Test chat completions with gemini-2.5-flash (simple message)
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -41,7 +41,7 @@
 ###
 
 ### Test chat completions with system prompt
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -60,7 +60,7 @@
 ###
 
 ### Test chat completions with conversation history
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -83,7 +83,7 @@
 ###
 
 ### Test with gemini-2.0-flash-thinking model
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -98,7 +98,7 @@
 ###
 
 ### Test with experimental model
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -116,7 +116,7 @@
 ###
 
 ### Test error handling - invalid model
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -131,7 +131,7 @@
 ###
 
 ### Test error handling - empty messages
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -142,7 +142,7 @@
 ###
 
 ### Test chat completions with image support (Base64)
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -169,7 +169,7 @@
 ###
 
 ### Test chat completions with image support (URL)
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -196,7 +196,7 @@
 ###
 
 ### Test chat completions with multiple images
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -227,7 +227,7 @@
 ###
 
 ### Test error handling - trying to use images with non-vision model
-POST https://gemini-cli-worker.gewoonjaap.workers.dev/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
diff --git a/build-and-push.bat b/build-and-push.bat
new file mode 100644
index 0000000..819616e
--- /dev/null
+++ b/build-and-push.bat
@@ -0,0 +1,17 @@
+@echo off
+setlocal
+
+REM Build the Docker image
+docker build -t drnit29/gemini-cli-openai:latest .
+if %errorlevel% neq 0 (
+    echo Docker build failed.
+    exit /b %errorlevel%
+)
+
+REM Push the Docker image to Docker Hub
+docker push drnit29/gemini-cli-openai:latest
+if %errorlevel% neq 0 (
+    echo Docker push failed.
+    exit /b %errorlevel%
+)
+
+echo Successfully built and pushed drnit29/gemini-cli-openai:latest
+endlocal
diff --git a/build-and-push.sh b/build-and-push.sh
new file mode 100755
index 0000000..7043818
--- /dev/null
+++ b/build-and-push.sh
@@ -0,0 +1,10 @@
+#!/bin/sh
+set -e
+
+# Build the Docker image
+docker build -t drnit29/gemini-cli-openai:latest .
+
+# Push the Docker image to Docker Hub
+docker push drnit29/gemini-cli-openai:latest
+
+echo "Successfully built and pushed drnit29/gemini-cli-openai:latest"
diff --git a/docker-compose.coolify.yml b/docker-compose.coolify.yml
new file mode 100644
index 0000000..20251c8
--- /dev/null
+++ b/docker-compose.coolify.yml
@@ -0,0 +1,26 @@
+# This file is designed for deployment on Coolify.
+# Environment variables should be set in the Coolify UI.
+# Coolify will automatically provision a network for these services.
+version: '3.8'
+
+services:
+  app:
+    image: drnit29/gemini-cli-openai:latest
+    ports:
+      - "3000:3000"
+    environment:
+      # Coolify injects PORT, but we can set a default.
+      - PORT=3000
+      # This should point to the Redis service defined below.
+      - REDIS_URL=redis://redis:6379
+      # All other environment variables (GCP_SERVICE_ACCOUNT, OPENAI_API_KEY, etc.)
+      # should be configured as secrets in your Coolify project settings.
+    depends_on:
+      - redis
+    restart: unless-stopped
+
+  redis:
+    image: redis:7-alpine
+    restart: unless-stopped
+    # Coolify can manage persistence for you, but you can define a volume if needed.
+    # volumes:
+    #   - redis_data:/data
diff --git a/docker-compose.yml b/docker-compose.yml
new file mode 100644
index 0000000..73d12d0
--- /dev/null
+++ b/docker-compose.yml
@@ -0,0 +1,19 @@
+version: '3.8'
+
+services:
+  app:
+    build: .
+    image: drnit29/gemini-cli-openai:latest
+    ports:
+      - "3000:3000"
+    env_file:
+      - .env
+    depends_on:
+      - redis
+    restart: unless-stopped
+
+  redis:
+    image: redis:7-alpine
+    restart: unless-stopped
+    volumes:
+      - redis_data:/data
+
+volumes:
+  redis_data:
diff --git a/package.json b/package.json
index 6806a6c..6c17e60 100644
--- a/package.json
+++ b/package.json
@@ -1,29 +1,29 @@
 {
-  "name": "gemini-cli-worker",
+  "name": "gemini-cli-openai",
   "version": "1.0.0",
   "private": true,
   "scripts": {
-    "deploy": "wrangler deploy",
-    "dev": "wrangler dev",
+    "start": "node dist/index.js",
+    "dev": "tsx watch src/index.ts",
     "format": "prettier --write src",
     "format:check": "prettier --check src",
-    "build": "wrangler deploy --dry-run --outdir=dist",
+    "build": "tsc",
     "lint": "eslint --ext .ts src",
     "lint:fix": "eslint --ext .ts src --fix"
   },
   "dependencies": {
+    "@hono/node-server": "^1.12.0",
+    "dotenv": "^16.4.5",
-    "hono": "^4.8.4"
+    "hono": "^4.8.4",
+    "redis": "^4.6.15"
   },
   "devDependencies": {
-    "@cloudflare/workers-types": "^4.20250705.0",
     "@eslint/eslintrc": "^3.3.1",
     "@eslint/js": "^9.30.1",
     "@types/node": "^24.0.4",
     "@typescript-eslint/eslint-plugin": "^8.35.0",
     "@typescript-eslint/parser": "^8.35.1",
     "eslint": "^9.30.0",
     "prettier": "^3.6.2",
-    "typescript": "^5.4.5",
-    "wrangler": "^4.23.0"
+    "tsx": "^4.16.2",
+    "typescript": "^5.4.5"
   }
 }
diff --git a/src/auth.ts b/src/auth.ts
index 7255b88..b862371 100644
--- a/src/auth.ts
+++ b/src/auth.ts
@@ -1,4 +1,5 @@
 import { Env, OAuth2Credentials } from "./types";
+import type { RedisClientType } from "redis";
 import {
 	CODE_ASSIST_ENDPOINT,
 	CODE_ASSIST_API_VERSION,
@@ -31,10 +32,12 @@
 export class AuthManager {
 	private env: Env;
 	private accessToken: string | null = null;
-
-	constructor(env: Env) {
+	private redis: RedisClientType;
+
+	constructor(env: Env, redis: RedisClientType) {
 		this.env = env;
+		this.redis = redis;
 	}
 
 	/**
@@ -47,15 +50,16 @@
 
 		try {
 			// First, try to get a cached token from KV storage
-			let cachedTokenData = null;
+			let cachedTokenData: CachedTokenData | null = null;
 
 			try {
-				const cachedToken = await this.env.GEMINI_CLI_KV.get(KV_TOKEN_KEY, "json");
-				if (cachedToken) {
-					cachedTokenData = cachedToken as CachedTokenData;
-					console.log("Found cached token in KV storage");
+				const cachedTokenString = await this.redis.get(KV_TOKEN_KEY);
+				if (cachedTokenString) {
+					cachedTokenData = JSON.parse(cachedTokenString) as CachedTokenData;
+					console.log("Found cached token in Redis");
 				}
-			} catch (kvError) {
-				console.log("No cached token found in KV storage or KV error:", kvError);
+			} catch (redisError) {
+				console.log("No cached token found in Redis or Redis error:", redisError);
 			}
 
 			// Check if cached token is still valid (with buffer)
@@ -75,7 +79,7 @@
 				console.log(`Original token is valid for ${Math.floor(timeUntilExpiry / 1000)} more seconds`);
 
 				// Cache the token in KV storage
-				await this.cacheTokenInKV(oauth2Creds.access_token, oauth2Creds.expiry_date);
+				await this.cacheTokenInRedis(oauth2Creds.access_token, oauth2Creds.expiry_date);
 				return;
 			}
 
@@ -118,12 +122,12 @@
 		console.log(`New token expires in ${refreshData.expires_in} seconds`);
 
 		// Cache the new token in KV storage
-		await this.cacheTokenInKV(refreshData.access_token, expiryTime);
+		await this.cacheTokenInRedis(refreshData.access_token, expiryTime);
 	}
 
 	/**
 	 * Cache the access token in KV storage.
 	 */
-	private async cacheTokenInKV(accessToken: string, expiryDate: number): Promise<void> {
+	private async cacheTokenInRedis(accessToken: string, expiryDate: number): Promise<void> {
 		try {
 			const tokenData = {
 				access_token: accessToken,
@@ -135,29 +139,27 @@
 			const ttlSeconds = Math.floor((expiryDate - Date.now()) / 1000) - 300; // 5 minutes buffer
 
 			if (ttlSeconds > 0) {
-				await this.env.GEMINI_CLI_KV.put(KV_TOKEN_KEY, JSON.stringify(tokenData), {
-					expirationTtl: ttlSeconds
+				await this.redis.set(KV_TOKEN_KEY, JSON.stringify(tokenData), {
+					EX: ttlSeconds
 				});
-				console.log(`Token cached in KV storage with TTL of ${ttlSeconds} seconds`);
+				console.log(`Token cached in Redis with TTL of ${ttlSeconds} seconds`);
 			} else {
-				console.log("Token expires too soon, not caching in KV");
+				console.log("Token expires too soon, not caching in Redis");
 			}
-		} catch (kvError) {
-			console.error("Failed to cache token in KV storage:", kvError);
+		} catch (redisError) {
+			console.error("Failed to cache token in Redis:", redisError);
 			// Don't throw an error here as the token is still valid, just not cached
 		}
 	}
 
 	/**
-	 * Clear cached token from KV storage.
+	 * Clear cached token from Redis.
 	 */
 	public async clearTokenCache(): Promise<void> {
 		try {
-			await this.env.GEMINI_CLI_KV.delete(KV_TOKEN_KEY);
-			console.log("Cleared cached token from KV storage");
-		} catch (kvError) {
-			console.log("Error clearing KV cache:", kvError);
+			await this.redis.del(KV_TOKEN_KEY);
+			console.log("Cleared cached token from Redis");
+		} catch (redisError) {
+			console.log("Error clearing Redis cache:", redisError);
 		}
 	}
 
@@ -166,9 +168,10 @@
 	 */
 	public async getCachedTokenInfo(): Promise<TokenCacheInfo> {
 		try {
-			const cachedToken = await this.env.GEMINI_CLI_KV.get(KV_TOKEN_KEY, "json");
-			if (cachedToken) {
-				const tokenData = cachedToken as CachedTokenData;
+			const cachedTokenString = await this.redis.get(KV_TOKEN_KEY);
+			if (cachedTokenString) {
+				const tokenData = JSON.parse(cachedTokenString) as CachedTokenData;
 				const timeUntilExpiry = tokenData.expiry_date - Date.now();
 
 				return {
diff --git a/src/index.ts b/src/index.ts
index 0368142..9c99684 100644
--- a/src/index.ts
+++ b/src/index.ts
@@ -1,10 +1,14 @@
+import "dotenv/config";
+import { serve } from "@hono/node-server";
 import { Hono } from "hono";
-import { Env } from "./types";
+import { createClient } from "redis";
+import { Env, HonoContext } from "./types";
 import { OpenAIRoute } from "./routes/openai";
 import { DebugRoute } from "./routes/debug";
 import { openAIApiKeyAuth } from "./middlewares/auth";
 import { loggingMiddleware } from "./middlewares/logging";
 
 /**
- * Gemini CLI OpenAI Worker
+ * Gemini CLI OpenAI
  *
- * A Cloudflare Worker that provides OpenAI-compatible API endpoints
- * for Google's Gemini models via the Gemini CLI OAuth flow.
+ * Provides OpenAI-compatible API endpoints for Google's Gemini models
+ * via the Gemini CLI OAuth flow, running as a Node.js server.
  *
  * Features:
  * - OpenAI-compatible chat completions and model listing
@@ -15,10 +19,23 @@
  */
 
 // Create the main Hono app
-const app = new Hono<{ Bindings: Env }>();
+const app = new Hono<HonoContext>();
 
 // Add logging middleware
 app.use("*", loggingMiddleware);
+
+// Redis client setup and middleware
+const redisClient = createClient({
+	url: process.env.REDIS_URL || "redis://localhost:6379"
+});
+redisClient.on("error", (err) => console.log("Redis Client Error", err));
+(async () => {
+	await redisClient.connect();
+})();
+app.use("*", async (c, next) => {
+	c.set("redis", redisClient);
+	await next();
+});
 
 // Add CORS headers for all requests
 app.use("*", async (c, next) => {
@@ -45,7 +62,7 @@
 
 // Root endpoint - basic info about the service
 app.get("/", (c) => {
-	const requiresAuth = !!c.env.OPENAI_API_KEY;
+	const requiresAuth = !!process.env.OPENAI_API_KEY;
 
 	return c.json({
 		name: "Gemini CLI OpenAI Worker",
@@ -72,6 +89,13 @@
 	return c.json({ status: "ok", timestamp: new Date().toISOString() });
 });
 
-export default app;
+const port = process.env.PORT ? parseInt(process.env.PORT, 10) : 3000;
+console.log(`Server is running on port ${port}`);
+
+serve({
+	fetch: app.fetch,
+	port
+});
+
+export default app; // For testing or other integrations
 
diff --git a/src/middlewares/auth.ts b/src/middlewares/auth.ts
index 548c080..42f638f 100644
--- a/src/middlewares/auth.ts
+++ b/src/middlewares/auth.ts
@@ -1,10 +1,10 @@
 import { MiddlewareHandler } from "hono";
-import { Env } from "../types";
+import { HonoContext } from "../types";
 
 /**
  * Middleware to enforce OpenAI-style API key authentication if OPENAI_API_KEY is set in the environment.
  * Checks for 'Authorization: Bearer <key>' header on protected routes.
  */
-export const openAIApiKeyAuth: MiddlewareHandler<{ Bindings: Env }> = async (c, next) => {
+export const openAIApiKeyAuth: MiddlewareHandler<HonoContext> = async (c, next) => {
 	// Skip authentication for public endpoints
 	const publicEndpoints = ["/", "/health"];
 	if (publicEndpoints.some((endpoint) => c.req.path === endpoint)) {
@@ -13,7 +13,7 @@
 	}
 
 	// If OPENAI_API_KEY is set in environment, require authentication
-	if (c.env.OPENAI_API_KEY) {
+	if (process.env.OPENAI_API_KEY) {
 		const authHeader = c.req.header("Authorization");
 
 		if (!authHeader) {
@@ -43,7 +43,7 @@
 		}
 
 		const providedKey = match[1];
-		if (providedKey !== c.env.OPENAI_API_KEY) {
+		if (providedKey !== process.env.OPENAI_API_KEY) {
 			return c.json(
 				{
 					error: {
diff --git a/src/routes/debug.ts b/src/routes/debug.ts
index 6831d1d..934c9c1 100644
--- a/src/routes/debug.ts
+++ b/src/routes/debug.ts
@@ -1,15 +1,15 @@
 import { Hono } from "hono";
-import { Env } from "../types";
+import { Env, HonoContext } from "../types";
 import { AuthManager } from "../auth";
 import { GeminiApiClient } from "../gemini-client";
 
 /**
  * Debug and testing routes for troubleshooting authentication and API functionality.
  */
-export const DebugRoute = new Hono<{ Bindings: Env }>();
+export const DebugRoute = new Hono<HonoContext>();
 
 // Check KV cache status
 DebugRoute.get("/cache", async (c) => {
 	try {
-		const authManager = new AuthManager(c.env);
+		const authManager = new AuthManager(process.env as Env, c.get("redis"));
 		const cacheInfo = await authManager.getCachedTokenInfo();
 
 		// Remove sensitive information from the response
@@ -41,7 +41,7 @@
 DebugRoute.post("/token-test", async (c) => {
 	try {
 		console.log("Token test endpoint called");
-		const authManager = new AuthManager(c.env);
+		const authManager = new AuthManager(process.env as Env, c.get("redis"));
 
 		// Test authentication only
 		await authManager.initializeAuth();
@@ -68,8 +68,8 @@
 DebugRoute.post("/test", async (c) => {
 	try {
 		console.log("Test endpoint called");
-		const authManager = new AuthManager(c.env);
-		const geminiClient = new GeminiApiClient(c.env, authManager);
+		const authManager = new AuthManager(process.env as Env, c.get("redis"));
+		const geminiClient = new GeminiApiClient(process.env as Env, authManager);
 
 		// Test authentication
 		await authManager.initializeAuth();
diff --git a/src/routes/openai.ts b/src/routes/openai.ts
index 8e4587c..673d611 100644
--- a/src/routes/openai.ts
+++ b/src/routes/openai.ts
@@ -1,5 +1,5 @@
 import { Hono } from "hono";
-import { Env, ChatCompletionRequest, ChatCompletionResponse } from "../types";
+import { Env, ChatCompletionRequest, ChatCompletionResponse, HonoContext } from "../types";
 import { geminiCliModels, DEFAULT_MODEL, getAllModelIds } from "../models";
 import { OPENAI_MODEL_OWNER } from "../config";
 import { DEFAULT_THINKING_BUDGET } from "../constants";
@@ -10,7 +10,7 @@
 /**
  * OpenAI-compatible API routes for models and chat completions.
  */
-export const OpenAIRoute = new Hono<{ Bindings: Env }>();
+export const OpenAIRoute = new Hono<HonoContext>();
 
 // List available models
 OpenAIRoute.get("/models", async (c) => {
@@ -36,7 +36,7 @@
 		const stream = body.stream !== false;
 
 		// Check environment settings for real thinking
-		const isRealThinkingEnabled = c.env.ENABLE_REAL_THINKING === "true";
+		const isRealThinkingEnabled = process.env.ENABLE_REAL_THINKING === "true";
 		const includeReasoning = isRealThinkingEnabled; // Automatically enable reasoning when real thinking is enabled
 		const thinkingBudget = body.thinking_budget ?? DEFAULT_THINKING_BUDGET; // Default to dynamic allocation
 
@@ -101,8 +101,8 @@
 		});
 
 		// Initialize services
-		const authManager = new AuthManager(c.env);
-		const geminiClient = new GeminiApiClient(c.env, authManager);
+		const authManager = new AuthManager(process.env as Env, c.get("redis"));
+		const geminiClient = new GeminiApiClient(process.env as Env, authManager);
 
 		// Test authentication first
 		try {
diff --git a/src/types.ts b/src/types.ts
index 7292150..b0e6e76 100644
--- a/src/types.ts
+++ b/src/types.ts
@@ -1,13 +1,25 @@
+import type { RedisClientType } from "redis";
+
 // --- Environment Variable Typings ---
 export interface Env {
 	GCP_SERVICE_ACCOUNT: string; // Now contains OAuth2 credentials JSON
 	GEMINI_PROJECT_ID?: string;
-	GEMINI_CLI_KV: KVNamespace; // Cloudflare KV for token caching
 	OPENAI_API_KEY?: string; // Optional API key for authentication
 	ENABLE_FAKE_THINKING?: string; // Optional flag to enable fake thinking output (set to "true" to enable)
 	ENABLE_REAL_THINKING?: string; // Optional flag to enable real Gemini thinking output (set to "true" to enable)
 	STREAM_THINKING_AS_CONTENT?: string; // Optional flag to stream thinking as content with <thinking> tags (set to "true" to enable)
+	PORT?: string;
+	REDIS_URL?: string;
 }
+
+// --- Hono Context Typings ---
+export type HonoContext = {
+	Bindings: Env;
+	Variables: {
+		redis: RedisClientType;
+	};
+};
 
 // --- OAuth2 Credentials Interface ---
 export interface OAuth2Credentials {
diff --git a/stream-test.http b/stream-test.http
index 3b05481..e656e18 100644
--- a/stream-test.http
+++ b/stream-test.http
@@ -1,5 +1,5 @@
 ### Test non-streaming chat completion
-POST http://localhost:8787/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -15,7 +15,7 @@
 ###
 
 ### Test streaming chat completion (default behavior)
-POST http://localhost:8787/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
@@ -31,7 +31,7 @@
 ###
 
 ### Test streaming chat completion (without explicit stream parameter - should default to true)
-POST http://localhost:8787/v1/chat/completions
+POST http://localhost:3000/v1/chat/completions
 Content-Type: application/json
 
 {
diff --git a/tsconfig.json b/tsconfig.json
index 6199468..959e455 100644
--- a/tsconfig.json
+++ b/tsconfig.json
@@ -3,13 +3,14 @@
     "target": "esnext",
     "module": "esnext",
     "moduleResolution": "node",
+    "outDir": "./dist",
     "lib": ["esnext"],
     "strict": true,
     "esModuleInterop": true,
     "skipLibCheck": true,
-    "forceConsistentCasingInFileNames": true,
-    "types": ["@cloudflare/workers-types"]
+    "forceConsistentCasingInFileNames": true
   },
   "include": ["src"],
   "exclude": ["node_modules"]
 }
diff --git a/wrangler.toml b/wrangler.toml
deleted file mode 100644
index 281a700..0000000
--- a/wrangler.toml
+++ /dev/null
@@ -1,23 +0,0 @@
-name = "gemini-cli-worker"
-main = "src/index.ts"
-compatibility_date = "2024-09-23"
-
-compatibility_flags = ["nodejs_compat"] # Required for the 'google-auth-library'
-
-# --- KV Namespaces ---
-kv_namespaces = [
-  { binding = "GEMINI_CLI_KV", id = "6c761101b4a34c089e6a123f2782c5fd" }
-]
-
-# --- Environment Variables ---
-# [vars]
-#
-# # REQUIRED: Your oauth_creds.json file content as a single-line string.
-# # Get this file by authenticating with the Gemini CLI (`gemini auth`).
-# # It's usually located at ~/.gemini/oauth_creds.json on your local machine.
-# GOOGLE_OAUTH_CREDS_JSON = '{"access_token":"ya29...","refresh_token":"1//...","scope":"...","token_type":"Bearer","expiry_date":1719...}'
-#
-# # OPTIONAL (but recommended): Your Google Cloud Project ID.
-# # Providing this skips the automatic discovery process, making the worker faster and more reliable.
-# # If omitted, the worker will attempt to discover it on the first run.
-# GEMINI_PROJECT_ID = "your-google-cloud-project-id"
-
-# wrangler.toml (wrangler v3.88.0^)
-[observability.logs]
-enabled = true

```